{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rabeya/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix as cmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a object-oriented programming approach to creating a \n",
    "# classifier based on Bayesian inference\n",
    "\n",
    "# a bayesian classifier must: \n",
    "# 1) take in as input training and test data,\n",
    "# 2) calculate \"priors\" of the classes based on training data,\n",
    "# 3) decide on a way to calculate likelihoods (KDE density or Gaussian mixture)\n",
    "# 4) finally, do posterior odds ratio prediction (or probabilities) by combining\n",
    "# priors with likelihoods \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name: auxiliary_functions.py \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def find_best_params(X, model_inst, param_grid):\n",
    "    if ( isinstance(model_inst, sklearn.neighbors.KernelDensity) ):\n",
    "        grid = GridSearchCV( estimator=model_inst, \n",
    "                                param_grid=param_grid, n_jobs=-1 )\n",
    "        grid.fit(X)\n",
    "        best_model = sklearn.neighbors.KernelDensity(kernel='gaussian', grid.best_params[list(param_grid)[0]])\n",
    "        best_model.fit(X)\n",
    "    elif ( isinstance(model_inst, sklearn.mixture.GaussianMixture) ):\n",
    "        grid = GridSearchCV( estimator=model_inst, \n",
    "                                param_grid=param_grid, n_jobs=-1 )\n",
    "        grid.fit(X)\n",
    "        best_model = sklearn.mixture.GaussianMixture(n_components=grid.best_params[list(param_grid)[0]])\n",
    "        best_model.fit(X)\n",
    "    else:\n",
    "        return 'Error handling user-inputted model instance name.'\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def is_tabular(X):\n",
    "    # check the dimensions of the data-matrix\n",
    "    # if it is 2-dimensional numpy array, its tabular\n",
    "    if (X.ndim == 2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "  \n",
    "    \n",
    "def is_image(X):\n",
    "    # check the dimensions of the data-matrix\n",
    "    # if it is 3-dimensional numpy array, its image\n",
    "    if (X.ndim == 3):\n",
    "        return True\n",
    "    else:\n",
    "        return False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file name: model_tech.py\n",
    "\n",
    "#from auxiliary_functions import *\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "class LikelihoodModel:\n",
    "    def __init__(self, X, y, model_type):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.categories = np.unique(y)\n",
    "        self.model_dict = None \n",
    "        self.model_type = model_type # the model inputted here \n",
    "    \n",
    "    def fit(self):\n",
    "        category_models = []\n",
    "        for category in self.categories:\n",
    "            category_index = np.where( self.y==category )\n",
    "            X_category = self.X[category_index]\n",
    "            # get input about parameter grid used on model\n",
    "            # now check for the model type\n",
    "            if ( self.model_type is sklearn.neighbors.KernelDensity ):\n",
    "                # fill in parameter dictionary\n",
    "                key = input('Enter parameter name: ')\n",
    "                start = float(input('Enter start value: '))\n",
    "                end = float(input('Enter end value: '))\n",
    "                step = float(input('# of points? '))\n",
    "                param_grid = {key: np.logspace(start, end, step)}\n",
    "                # grid-search\n",
    "                optimal_model = find_best_params( X_category, KernelDensity(kernel='gaussian'), param_grid )\n",
    "                category_models.append( optimal_model )\n",
    "            # otherwise if the model is Gaussian Mixture \n",
    "            elif ( self.model_type is sklearn.mixture.GaussianMixture ):\n",
    "                # fill in parameter dictionary\n",
    "                key = input('Enter parameter name: ')\n",
    "                n_components = int(input('# of components? ')) \n",
    "                param_grid = {key: np.asarray(list(range(1, n_components+1))) }\n",
    "                # grid-search\n",
    "                optimal_model = find_best_params( X_category, GaussianMixture(n_components=n_components), param_grid )\n",
    "                category_models.append( optimal_model )    \n",
    "        # put all the models for each category into the model_dict instance variable        \n",
    "        self.model_dict = {('Model {i}'.format(i)):m for (i,m) in enumerate(category_models)}\n",
    "        \n",
    "    def visualize(self):\n",
    "        # first, choose a model from the self.model_dict\n",
    "        \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name: features_tech.py\n",
    "\n",
    "import umap\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from auxiliary_functions import *\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.features_extracted = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def train_test_split(self, train_percent=70):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = sklearn.model_selection.train_test_split(self.X, self.y, \n",
    "                                                                             train_size=(train_percent/100), random_state=2)\n",
    "    def get_features(self, num_features):\n",
    "        if is_tabular(self.X_train):\n",
    "            # if data is tabular\n",
    "            # we can use random forest feature_importance method\n",
    "            if ( numpy.isnan(self.X_train).any() or numpy.isnan(self.y_train).any() ):\n",
    "                # first check if any missing values\n",
    "                # so if either condition is true, there are missing values\n",
    "                return 'Your data-matrix or target-class has missing values. Please fill them in first.'\n",
    "            else:\n",
    "                # data is all complete, do mutual information feature extractor\n",
    "                # selectKbest\n",
    "                KBest = SelectKBest(mutual_info_classif, _?__)\n",
    "                transformed = KBest.fit_transform(self.X_train, self.y_train)\n",
    "                self.features_extracted = transformed\n",
    "                print('{} best features extracted!'.format(num_features))\n",
    "        elif is_image(self.X_train):\n",
    "            # we can only use UMAP\n",
    "            # we need to first reshape self.X_train from 3D numpy array to 2D array\n",
    "            X_train_reshaped = np.reshape(self.X_train, (len(self.X_train), -1))\n",
    "            # then use UMAP\n",
    "            mapper = umap.UMAP(n_components=num_features, random_state=2)\n",
    "            mapper.fit(X_train_reshaped)\n",
    "            umap_features = mapper.transform(X_train_reshaped)\n",
    "            self.features_extracted = umap_features\n",
    "            print('{} best features extracted!'.format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4245432b3c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "# filename: main.py\n",
    "\n",
    "#from feature_tech import FeatureExtractor\n",
    "#from models_tech import LikelihoodModel \n",
    "# from auxiliary_func import *\n",
    "\n",
    "\n",
    "\n",
    "class BayesClassifier:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        image_data = None\n",
    "        tabular_data = None\n",
    "        self.categories = np.unique(y)\n",
    "        self.log_priors = None\n",
    "        self.log_likelihoods = None\n",
    "        self.log_posteriors = None\n",
    "        self.X_features = None\n",
    "        self.model_dict = None\n",
    "        if is_image(X):\n",
    "            image_data = True\n",
    "        elif is_tabular(X):\n",
    "            tabular_data = True\n",
    "        else:\n",
    "            return 'Please load in either image or tabular data.'\n",
    "      \n",
    "    \n",
    "    def preprocess(self, n_features):\n",
    "        Fext = FeatureExtractor(self.X, self.y)\n",
    "        Fext.train_test_split()\n",
    "        Fext.get_features(num_features=n_features)\n",
    "        self.X_features = Fext.features_extracted\n",
    "        self.y_train, self.y_test, self.X_test = Fext.y_train, Fext.y_test, Fext.X_test  \n",
    "            \n",
    "    def use_model(self, model_type):\n",
    "        # model_type is the type of model. KernelDensity or GaussianMixture (from sklearn) work as values\n",
    "        if ( model_type is sklearn.neighbors.KernelDensity ):\n",
    "            Model = LikelihoodModel(self.X_features, self.y_train, sklearn.neighbors.KernelDensity)\n",
    "            Model.fit()\n",
    "            self.model_dict = Model.model_dict\n",
    "        elif ( model_type is sklearn.mixture.GaussianMixture ):\n",
    "            Model = LikelihoodModel(self.X_features, self.y_train, sklearn.mixture.GaussianMixture)\n",
    "            Model.fit()\n",
    "            self.model_dict = Model.model_dict\n",
    "    \n",
    "    def predict_proba(self):\n",
    "        # this function will do the actual posterior probability  prediction \n",
    "        # we add the log_priors + log_likelihoods = log_posteriors (and normalize) \n",
    "        if image_data:\n",
    "            X_test_reshaped = np.reshape(self.X_test, (len(self.X_test), -1))\n",
    "            # we need to first reshape self.X_train from 3D numpy array to 2D array\n",
    "            # for each row (data-point) in the test data-matrix\n",
    "            self.log_likelihoods = np.asarray([model.score(X_test_reshaped) for model in self.model_dict])\n",
    "            self.log_priors = np.log(pd.Series(self.y_train).value_counts(normalize=True).values)\n",
    "            norm = np.add(self.log_likelihoods, self.log_priors)\n",
    "            self.posteriors = norm/np.sum(norm)     \n",
    "        elif tabular_data:\n",
    "            # we do NOT need to reshape anything since its already like a table\n",
    "            self.log_likelihoods = np.asarray([model.score(X_test_reshaped) for model in self.model_dict])\n",
    "            self.log_priors = np.log(np.asarray(pd.Series(self.y_train).value_counts(normalize=True).values))\n",
    "            normalized = np.add(self.log_likelihoods, self.log_priors)\n",
    "            # normalized is a np-array\n",
    "            self.log_posteriors = normalized/np.sum(normalized)\n",
    "                                     \n",
    "    def predict_class(self):\n",
    "        # we'll use the results of the predict_proba() function \n",
    "        # self.log_posteriors contains the Logarithm(posterior probabilities) already normalized!\n",
    "        # so we just pick the index (in the np-array) corresponding to highest posterior, report that\n",
    "        # (along with its % probability)\n",
    "        posterior_copy = \n",
    "        index_firstplace = self.log_posteriors.index(max(self.log_posteriors))\n",
    "        index_secondplace = \n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "if name == __main__:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.asarray([2,1,4,5,4,8,7,0])\n",
    "M = L.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 4, 5, 4, 8, 7, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 4, 5, 4, 8, 7, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'delete'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9994186543d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'delete'"
     ]
    }
   ],
   "source": [
    "M.delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20.3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
